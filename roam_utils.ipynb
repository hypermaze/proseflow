{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp roam_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from proseflow.aws_utils import *\n",
    "from proseflow.load import *\n",
    "from proseflow.text import *\n",
    "from proseflow.utils import pipe\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for s_func in STRING_FUNCS: #PYTHON MAGIC\n",
    "    exec(\"%s=getattr(str, s_func)\" %s_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"roam-export\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roam_graph = read_json_from_s3(bucket=BUCKET, key=\"scify.json\").get(\"data\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "URL_REGEX = '\\(?((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\\)?'\n",
    "\n",
    "def remove_html_tags(form, content=\"HTML_TAGS\"):\n",
    "    html_tags = re.compile('<.*?>')\n",
    "    return re.sub(html_tags, '', form)\n",
    "\n",
    "def remove_buttons(form):\n",
    "    button_elems = re.compile('\\{\\{\\[\\[(TODO|DONE|slider)\\]\\]\\}\\}')\n",
    "    return re.sub(button_elems, '', form)\n",
    "\n",
    "def remove_url(form):\n",
    "    url = re.compile(URL_REGEX)\n",
    "    return re.sub(url, '', form)\n",
    "\n",
    "def remove_attr(form):\n",
    "    attr = re.compile('^[^:\\r\\n]+:*')\n",
    "    return re.sub(attr, '', form)\n",
    "\n",
    "def replace_block_ref(form, lookup):\n",
    "    \"\"\"good job here... my implementation in js was much worse\"\"\"\n",
    "    block_ref = re.compile('\\(\\((.*)\\)\\)')\n",
    "    block = re.search(block_ref, form)\n",
    "    if block: \n",
    "        return lookup.get(re.sub('[()]', '', block.group(0)))\n",
    "    return form\n",
    "\n",
    "def remove_duplicates(form: Iterable):\n",
    "    if isinstance(form, list):\n",
    "        return list(set(form))\n",
    "    return form\n",
    "\n",
    "clean_sentence = pipe(\n",
    "                      replace_block_ref,\n",
    "                      remove_buttons,\n",
    "                      remove_html_tags,\n",
    "                      remove_url,\n",
    "#                       remove_attr, #attention !!! buggy!\n",
    "                      (replace, \"  \", \" \"),\n",
    "                      (replace, \"[\", \"\"),\n",
    "                      (replace, \"]\", \"\"), \n",
    "                      (replace, \"#\", \"\"),\n",
    "                      (replace, \"`\", \"\"),\n",
    "                      (replace, \"__\", \"\"),\n",
    "                      (replace, \"~~\", \"\"),\n",
    "                      (replace, \"**\", \"\"),\n",
    "                      (replace, \"^^\", \"\"),\n",
    "                      strip, \n",
    "                      lower\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(remove_buttons(\"{{[[TODO]]}} watch at least 5 videos of the course on bio\"), \" watch at least 5 videos of the course on bio\")\n",
    "test_eq(remove_buttons(\"{{[[DONE]]}} watch at least 5 videos of the course on bio\"), \" watch at least 5 videos of the course on bio\")\n",
    "test_eq(remove_url(\"[How to take smart notes video](https://vimeo.com/275530205)\"), \"[How to take smart notes video]\")\n",
    "test_eq(remove_url(\"How to take smart notes video https://vimeo.com/275530205\"), \"How to take smart notes video \")\n",
    "test_eq(remove_attr(\"tag:: #programming\"), \" #programming\")\n",
    "# print(replace_block_ref(\"First, extract all the relevant sentences using the words that were used in the dataset from ((nC-tI-yaD))\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def is_too_short(s, length=10): \n",
    "    return len(s.split(\" \")) < length\n",
    "\n",
    "stop_symbols = [\"TODO\", \"DONE\", \"::\", \"```\", \"![\"]\n",
    "def has_stop_symbols(s): \n",
    "    return any([symbol in s for symbol in stop_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def roam_graph_to_blocks(roam_graph):\n",
    "    roam_blocks_map = {}\n",
    "\n",
    "    def extract_strings(roam_block):\n",
    "        if type(roam_block) == list:\n",
    "            roam_block = roam_block[0]\n",
    "        if roam_block.get(\"string\") and roam_block.get(\"uid\"):\n",
    "            roam_blocks_map[roam_block.get(\"uid\")] = roam_block.get(\"string\")\n",
    "        if roam_block.get(\"children\"):\n",
    "            for child in roam_block.get(\"children\"):\n",
    "                extract_strings(child)\n",
    "                \n",
    "    for block in roam_graph:\n",
    "        extract_strings(block)\n",
    "        \n",
    "    return roam_blocks_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roam_graph = read_json_from_s3(bucket=BUCKET, key=\"scify.json\").get(\"data\")[:10]\n",
    "roam_blocks = roam_graph_to_blocks(test_roam_graph)\n",
    "test_vectors = np.arange(len(roam_blocks))\n",
    "\n",
    "roam_block_uid = list(roam_blocks.keys())[0]\n",
    "roam_block_sentence = list(roam_blocks.values())[0]\n",
    "\n",
    "test_eq_type(type(roam_block_uid), str)\n",
    "test_eq_type(type(roam_block_sentence), str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def roam_blocks_to_embeddings_index(roam_blocks, vectors):\n",
    "    roam_embeddings = []\n",
    "    \n",
    "    for i, (uid, sentence) in enumerate(roam_blocks.items()):\n",
    "        if not is_too_short(sentence) and not has_stop_symbols(sentence):\n",
    "#             cleaned_sentence = clean_sentence(sentence)\n",
    "            roam_embeddings.append({\n",
    "                \"uid\": uid,\n",
    "                \"sentence\": sentence,\n",
    "                \"embedding\": vectors[i].tolist()\n",
    "            })\n",
    "    \n",
    "    return roam_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roam_blocks_to_embeddings_index(roam_blocks, vectors):\n",
    "    roam_embeddings = []\n",
    "    \n",
    "    for i, (uid, sentence) in enumerate(roam_blocks.items()):\n",
    "        if not is_too_short(sentence) and not has_stop_symbols(sentence):\n",
    "#             cleaned_sentence = clean_sentence(sentence)\n",
    "            roam_embeddings.append({\n",
    "                \"uid\": uid,\n",
    "                \"sentence\": sentence,\n",
    "                \"embedding\": vectors[i].tolist()\n",
    "            })\n",
    "    \n",
    "    return roam_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> 3391fddbf684cbbf0770a6ee8bad0bc2936ea471`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = roam_blocks_to_embeddings_index(roam_blocks, test_vectors)\n",
    "test_eq(list(index_list[0].keys()), [\"uid\", \"sentence\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
