{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections.abc import Iterable\n",
    "from io import BytesIO\n",
    "from typing import Dict\n",
    "\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "import stanza\n",
    "from dotenv import load_dotenv\n",
    "from multipledispatch import dispatch\n",
    "from pandas import DataFrame\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from spacy_stanza import StanzaLanguage\n",
    "from textacy.corpus import Corpus\n",
    "from typeguard import typechecked\n",
    "\n",
    "from proseflow.spec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # TODO: [Markus -> use func.signature()]\n",
    "    # gspreadsheet\n",
    "    # csv\n",
    "    # tsv\n",
    "    # pubmed articles\n",
    "    # wikipedia\n",
    "    # url\n",
    "    # load spacy_corpus\n",
    "    # annotations\n",
    "    # BRAT\n",
    "    # Resource = Union[URL, str, email]\n",
    "# ? @typecheck is pointless here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> This module loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function proseflow.spec.gsheet_to_df(worksheet) -> pandas.core.frame.DataFrame>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsheet_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIR_PATH = os.path.dirname(os.path.realpath(__file__))\n",
    "load_dotenv()\n",
    "env_debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: -> converter\n",
    "# Example: https://docs.google.com/spreadsheets/d/1N_aANmDaosjAlodJ5nMNVPfe6REsDtsNYHj_ltH3Q_0/edit?usp=drive_web&ouid=112317186249575590696\n",
    "#export\n",
    "@typechecked\n",
    "def _load_gsheet(\n",
    "    url: str,\n",
    "    sheet_number: int = 0,\n",
    "    credential_path: str = os.getenv(\"GSHEET_CREDENTIALS\"),\n",
    "    **kwargs,\n",
    ") -> GSHEET:\n",
    "    if not credential_path:\n",
    "        raise Exception(\"Add the $GSHEET_CREDENTIALS variable to your .env file.\")\n",
    "    gc = gspread.service_account(filename=credential_path)\n",
    "    wb = gc.open_by_url(url)\n",
    "    worksheet = wb.get_worksheet(sheet_number)\n",
    "\n",
    "    return worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_corpus(nlp, path):\n",
    "    corpus = Corpus(nlp).load(nlp, path)\n",
    "    for label in labels:\n",
    "        nlp.vocab.strings.add(label)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _load_json(path: str, **kwargs):\n",
    "    with open(path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def _load_txt(path: str, **kwargs):\n",
    "    with open(path, \"r\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Respirology. 2016 Jul;21(5):821-33. doi: 10.1111/resp.12789. Epub 2016 Apr 21.\\n\\nImmunotherapy for lung cancer.\\n\\nSteven A(1)(2), Fisher SA(1)(2), Robinson BW(1)(2).\\n\\nAuthor information:\\n(1)School of Medicine and Pharmacology, University of Western Australia, \\nCrawley, Western Australia, Australia.\\n(2)National Centre for Asbestos Related Diseases (NCARD), Perth, Western \\nAustralia, Australia.\\n\\nTreatment of lung cancer remains a challenge, and lung cancer is still the \\nleading cause of cancer-re'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = _load_txt(\"../data/cancer_abstracts.txt\")\n",
    "txt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: [Rico] make it work with \"stanza\" or \"sci-md\" strings\n",
    "#export\n",
    "@dispatch((spacy.language.Language, StanzaLanguage), str)\n",
    "def load(nlp, path):\n",
    "    return _load_corpus(nlp, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dispatch(Iterable)\n",
    "def load(resource, **kwargs):\n",
    "    \"\"\"All shapes become lists for further processing\n",
    "    #TODO: [Rico] -- a job for autoconvert?\n",
    "    \"\"\"\n",
    "    shape_iterable = convert(resource, source=type(resource), target=list)\n",
    "    return load(shape_iterable, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [Rico] cache all sane things\n",
    "#export\n",
    "@dispatch(list)\n",
    "def load(resource, **kwargs):\n",
    "    #! checks the type of the FIRST element (like an actual pmid, not a list of pmids)\n",
    "    shape = kwargs.get(\"input_type\") or infer_type(resource[0])\n",
    "    if shape == PUBMED_IDS:\n",
    "        content = kwargs.get(PUBMED_CONTENT) or \"ALL\"\n",
    "        if content == \"ABSTRACT\":\n",
    "            return _get_pubmed_abstracts(pmids=resource)\n",
    "        if content == \"INFO\":\n",
    "            return _get_pubmed_info(pmids=resource)\n",
    "        return _get_pubmed_records(pmids=resource)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_get_pubmed_abstracts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fde38cb973ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_get_pubmed_abstracts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_get_pubmed_abstracts' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_transformer(model_name):\n",
    "    # ! TODO: abstract so that it also works for Tensorflow, etc..; right now its only PyTorch\n",
    "    # TODO: make sure it actually loads a huggingface transformer and not the sentence transformer version\n",
    "    model_name = model_name.split(\":\")[1]\n",
    "\n",
    "    return models.Transformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_spacy(model_name: str = \"en_core_web_sm\", **kwargs) -> spacy.language.Language:\n",
    "    print(\"Loading SpaCy...\")\n",
    "    try:\n",
    "        nlp = spacy.load(model_name, **kwargs)\n",
    "    except OSError:\n",
    "        try:\n",
    "            spacy.cli.download(model_name)\n",
    "            nlp = spacy.load(model_name, **kwargs)\n",
    "        except:\n",
    "            print(\"Download the SpaCy model before trying to import it.\")\n",
    "            return None\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_stanza(\n",
    "    stanza_setup: Dict[str, str] = {\n",
    "        \"lang\": \"en\",\n",
    "        \"package\": \"genia\",\n",
    "        \"processors\": {\"ner\": \"bionlp13cg\"},\n",
    "    },\n",
    "    use_gpu: bool = True,\n",
    ") -> stanza.Pipeline:\n",
    "    # TODO: [RICO -> put use_gpu inside one config]\n",
    "    print(\"loading stanza\", stanza_setup)\n",
    "    try:\n",
    "        snlp = stanza.Pipeline(**stanza_setup, use_gpu=use_gpu)\n",
    "    except:\n",
    "        stanza.download(**stanza_setup)\n",
    "        snlp = stanza.Pipeline(**stanza_setup, use_gpu=use_gpu)\n",
    "\n",
    "    return snlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dispatch(str)  # dispatch decides if the load gets executed; the type level is more expressive\n",
    "def load(resource, *args, **kwargs):\n",
    "    \"\"\"This names the important args like config and credentials, but leaves options open\"\"\"\n",
    "\n",
    "\n",
    "    if resource.endswith(\".csv\"):\n",
    "        return pd.read_csv(resource)\n",
    "    if resource.endswith(\".tsv\"):\n",
    "        pass\n",
    "    if resource == \"some url\":\n",
    "        pass  # scrape (params:)\n",
    "    if resource.endswith(\".json\"):\n",
    "        return _load_json(resource)\n",
    "    if resource.endswith(\".txt\"):\n",
    "        return _load_txt(resource)\n",
    "\n",
    "    shape = kwargs.get(\"input_type\") or infer_type(resource)\n",
    "    print(shape, \"shape\", kwargs)\n",
    "    as_type = kwargs.get(\"as_type\")\n",
    "    should_convert = as_type is not None\n",
    "    if shape == GSHEET:\n",
    "        gs = _load_gsheet(resource, **kwargs)\n",
    "\n",
    "        # ! Don't Try to be smart here and use (should_convert and convert(...) -- there's problems with boolean\n",
    "        # operators and some types)\n",
    "        if should_convert:\n",
    "            gs = convert(gs, source=GSHEET, target=as_type)\n",
    "            if as_type == DataFrame and kwargs.get(\"columns\"):\n",
    "                gs = gs[kwargs.get(\"columns\")]\n",
    "        return gs\n",
    "    if shape == SPACY_MODEL:\n",
    "        return _load_spacy(resource, **kwargs)\n",
    "    if shape == STANZA_MODEL:\n",
    "        if as_type:\n",
    "            kwargs.pop(\"as_type\")\n",
    "        snlp = _load_stanza(**kwargs)\n",
    "        if as_type:\n",
    "            return convert(snlp, source=STANZA_MODEL, target=SPACY_MODEL)\n",
    "        return snlp\n",
    "    if shape == SENTENCE_TRANSFORMER:\n",
    "        return SentenceTransformer(resource)\n",
    "    if shape == TRANSFORMER:\n",
    "        transformer_model = _load_transformer(resource)\n",
    "        if as_type:\n",
    "            return convert(\n",
    "                transformer_model, source=TRANSFORMER, target=SENTENCE_TRANSFORMER\n",
    "            )\n",
    "        return transformer_model\n",
    "\n",
    "    return \"None found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Respirology. 2016 Jul;21(5):821-33. doi: 10.1111/resp.12789. Epub 2016 Apr 21.\\n\\nImmunotherapy for'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load(\"../data/cancer_abstracts.txt\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'re': '(zh|da|nl|en|fr|de|el|it|ja|lt|nb|pl|pt|ro|es|xx)[_(core|ent|ner)_(web|news|wiki|sci|craft|jnlpba|bc5cdr|bionlp13cg)_(sm|md|lg)]*$'} shape {}\n",
      "Loading SpaCy...\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/markus/Library/Caches/pypoetry/virtualenvs/proseflow-GKtXBSGs-py3.8/lib/python3.8/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/markus/Library/Caches/pypoetry/virtualenvs/proseflow-GKtXBSGs-py3.8/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "test_eq(type(load(\"en\")), spacy.lang.en.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'re': '(zh|da|nl|en|fr|de|el|it|ja|lt|nb|pl|pt|ro|es|xx)[_(core|ent|ner)_(web|news|wiki|sci|craft|jnlpba|bc5cdr|bionlp13cg)_(sm|md|lg)]*$'} shape {'disable': ['tagger', 'ner', 'parser']}\n",
      "Loading SpaCy...\n"
     ]
    }
   ],
   "source": [
    "test_eq(type(load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\", \"parser\"])), spacy.lang.en.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'SENTENCE_TRANSFORMER'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTENCE_TRANSFORMER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'SENTENCE_TRANSFORMER'} shape {'input_type': {'name': 'SENTENCE_TRANSFORMER'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer(\n",
       "    (auto_model): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Pooling()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load(\"distilbert-base-nli-mean-tokens\", input_type=SENTENCE_TRANSFORMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_eq(type(load(\"distilbert-base-nli-mean-tokens\", input_type=\"SENTENCE_TRANSFORMER\")), SentenceTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "@dispatch(int)\n",
    "def save(what, where):\n",
    "    # spacy_docs_to_corpus -> annotation\n",
    "    # csv\n",
    "    # tsv\n",
    "    # to_local (Binary, String, List[str], List[json], json, dict)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
