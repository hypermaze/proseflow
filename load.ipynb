{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6c9b593cc83c>:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Iterable\n",
    "from collections.abc import Iterable\n",
    "from io import BytesIO\n",
    "from typing import Dict\n",
    "\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "import stanza\n",
    "from dotenv import load_dotenv\n",
    "from multipledispatch import dispatch\n",
    "from pandas import DataFrame\n",
    "from sentence_transformers import models\n",
    "from spacy_stanza import StanzaLanguage\n",
    "from textacy.corpus import Corpus\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from proseflow.spec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # TODO: [Markus -> use func.signature()]\n",
    "    # gspreadsheet\n",
    "    # csv\n",
    "    # tsv\n",
    "    # pubmed articles\n",
    "    # wikipedia\n",
    "    # url\n",
    "    # load spacy_corpus\n",
    "    # annotations\n",
    "    # BRAT\n",
    "    # Resource = Union[URL, str, email]\n",
    "# ? @typecheck is pointless here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> This module loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function proseflow.spec.gsheet_to_df(worksheet) -> pandas.core.frame.DataFrame>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIR_PATH = os.path.dirname(os.path.realpath(__file__))\n",
    "load_dotenv()\n",
    "env_debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: -> converter\n",
    "# Example: https://docs.google.com/spreadsheets/d/1N_aANmDaosjAlodJ5nMNVPfe6REsDtsNYHj_ltH3Q_0/edit?usp=drive_web&ouid=112317186249575590696\n",
    "#export\n",
    "@typechecked\n",
    "def _load_gsheet(\n",
    "    url: str,\n",
    "    sheet_number: int = 0,\n",
    "    credential_path: str = os.getenv(\"GSHEET_CREDENTIALS\"),\n",
    "    **kwargs,\n",
    ") -> GSHEET:\n",
    "    if not credential_path:\n",
    "        raise Exception(\"Add the $GSHEET_CREDENTIALS variable to your .env file.\")\n",
    "    gc = gspread.service_account(filename=credential_path)\n",
    "    wb = gc.open_by_url(url)\n",
    "    worksheet = wb.get_worksheet(sheet_number)\n",
    "\n",
    "    return worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_corpus(nlp, path):\n",
    "    corpus = Corpus(nlp).load(nlp, path)\n",
    "    for label in labels:\n",
    "        nlp.vocab.strings.add(label)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: [Rico] make it work with \"stanza\" or \"sci-md\" strings\n",
    "#export\n",
    "@dispatch((spacy.language.Language, StanzaLanguage), str)\n",
    "def load(nlp, path):\n",
    "    return _load_corpus(nlp, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dispatch(Iterable)\n",
    "def load(resource, **kwargs):\n",
    "    \"\"\"All shapes become lists for further processing\n",
    "    #TODO: [Rico] -- a job for autoconvert?\n",
    "    \"\"\"\n",
    "    shape_iterable = convert(resource, source=type(resource), target=list)\n",
    "    return load(shape_iterable, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [Rico] cache all sane things\n",
    "#export\n",
    "@dispatch(list)\n",
    "def load(resource, **kwargs):\n",
    "    #! checks the type of the FIRST element (like an actual pmid, not a list of pmids)\n",
    "    shape = kwargs.get(\"input_type\") or infer_type(resource[0])\n",
    "    if shape == PUBMED_IDS:\n",
    "        content = kwargs.get(PUBMED_CONTENT) or \"ALL\"\n",
    "        if content == \"ABSTRACT\":\n",
    "            return _get_pubmed_abstracts(pmids=resource)\n",
    "        if content == \"INFO\":\n",
    "            return _get_pubmed_info(pmids=resource)\n",
    "        return _get_pubmed_records(pmids=resource)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_transformer(model_name):\n",
    "    # ! TODO: abstract so that it also works for Tensorflow, etc..; right now its only PyTorch\n",
    "    # TODO: make sure it actually loads a huggingface transformer and not the sentence transformer version\n",
    "    model_name = model_name.split(\":\")[1]\n",
    "\n",
    "    return models.Transformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_spacy(model_name: str = \"en_core_web_sm\") -> spacy.language.Language:\n",
    "    print(\"Loading SpaCy...\")\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        try:\n",
    "            spacy.cli.download(model_name)\n",
    "            nlp = spacy.load(model_name)\n",
    "        except:\n",
    "            print(\"Download the SpaCy model before trying to import it.\")\n",
    "            return None\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_stanza(\n",
    "    stanza_setup: Dict[str, str] = {\n",
    "        \"lang\": \"en\",\n",
    "        \"package\": \"genia\",\n",
    "        \"processors\": {\"ner\": \"bionlp13cg\"},\n",
    "    },\n",
    "    use_gpu: bool = True,\n",
    ") -> stanza.Pipeline:\n",
    "    # TODO: [RICO -> put use_gpu inside one config]\n",
    "    print(\"loading stanza\", stanza_setup)\n",
    "    try:\n",
    "        snlp = stanza.Pipeline(**stanza_setup, use_gpu=use_gpu)\n",
    "    except:\n",
    "        stanza.download(**stanza_setup)\n",
    "        snlp = stanza.Pipeline(**stanza_setup, use_gpu=use_gpu)\n",
    "\n",
    "    return snlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dispatch(str)  # dispatch decides if the load gets executed; the type level is more expressive\n",
    "def load(resource, *args, **kwargs):\n",
    "    \"\"\"This names the important args like config and credentials, but leaves options open\"\"\"\n",
    "\n",
    "\n",
    "    if resource.endswith(\".csv\"):\n",
    "        pass\n",
    "    if resource.endswith(\".tsv\"):\n",
    "        pass\n",
    "    if resource == \"some url\":\n",
    "        pass  # scrape (params:)\n",
    "\n",
    "    shape = kwargs.get(\"input_type\") or infer_type(resource)\n",
    "\n",
    "    as_type = kwargs.get(\"as_type\")\n",
    "    should_convert = as_type is not None\n",
    "    if shape == GSHEET:\n",
    "        gs = _load_gsheet(resource, **kwargs)\n",
    "\n",
    "        # ! Don't Try to be smart here and use (should_convert and convert(...) -- there's problems with boolean\n",
    "        # operators and some types)\n",
    "        if should_convert:\n",
    "            gs = convert(gs, source=GSHEET, target=as_type)\n",
    "            if as_type == DataFrame and kwargs.get(\"columns\"):\n",
    "                gs = gs[kwargs.get(\"columns\")]\n",
    "        return gs\n",
    "    if shape == SPACY_MODEL:\n",
    "        return _load_spacy(resource)\n",
    "    if shape == STANZA_MODEL:\n",
    "        if as_type:\n",
    "            kwargs.pop(\"as_type\")\n",
    "        snlp = _load_stanza(**kwargs)\n",
    "        if as_type:\n",
    "            return convert(snlp, source=STANZA_MODEL, target=SPACY_MODEL)\n",
    "        return snlp\n",
    "    if shape == TRANSFORMER:\n",
    "        transformer_model = _load_transformer(resource)\n",
    "        if as_type:\n",
    "            return convert(\n",
    "                transformer_model, source=TRANSFORMER, target=SENTENCE_TRANSFORMER\n",
    "            )\n",
    "        return transformer_model\n",
    "\n",
    "    return \"None found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpaCy...\n"
     ]
    }
   ],
   "source": [
    "test_eq(type(load(\"en\")), spacy.lang.en.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "==:\n1\n2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7699ddba0e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nbdev-template-Ma8X7uFe-py3.8/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest_eq\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"`test` that `a==b`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nbdev-template-Ma8X7uFe-py3.8/lib/python3.8/site-packages/fastcore/test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ==:\n1\n2"
     ]
    }
   ],
   "source": [
    "test_eq(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "@dispatch(int)\n",
    "def save(what, where):\n",
    "    # spacy_docs_to_corpus -> annotation\n",
    "    # csv\n",
    "    # tsv\n",
    "    # to_local (Binary, String, List[str], List[json], json, dict)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
