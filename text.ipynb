{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import listify\n",
    "import faiss\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "STRING_FUNCS = [\"capitalize\", \"count\", \"isalnum\", \"isalpha\", \"isascii\", \"isdecimal\", \"isdigit\", \"isidentifier\", \"islower\",\n",
    "                \"isnumeric\", \"isspace\", \"istitle\", \"isupper\", \"lower\", \"lstrip\", \"replace\", \"rstrip\", \"split\", \"strip\", \"upper\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: add convert so that i automatically converts a string/np array/dataframe/tensor into a list\n",
    "def text_to_vector(text_form, embedder):\n",
    "    embeddings = embedder.encode(text_form, convert_to_tensor=True)\n",
    "    return embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_index(vectors):\n",
    "    faiss_index = faiss.IndexFlatL2(len(vectors[0]))\n",
    "    faiss_index.add(vectors)\n",
    "\n",
    "#     print(faiss_index.ntotal)\n",
    "\n",
    "    return faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query_index(text, embedder, target_list, index, with_distance=False, k=10):\n",
    "    embedding = embedder.encode([text])\n",
    "    distances, indices = index.search(embedding, k)\n",
    "    if with_distance:\n",
    "        return [(target_list[index], distances[0][i]) for i, index in enumerate(indices[0])]\n",
    "    return [target_list[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypernyms(ngram: str):\n",
    "    \"\"\"\n",
    "    parent in an ontology\n",
    "    \"\"\"\n",
    "    if \"WORDNET\":\n",
    "        pass\n",
    "    if \"CORPUS_FILE\":\n",
    "        hearst_patterns(\"zxz\")\n",
    "\n",
    "\n",
    "def hearst_patterns(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    animals like a, b, c\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted aws_utils.ipynb.\n",
      "Converted biomedrel.ipynb.\n",
      "Converted cancer_10k.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted load.ipynb.\n",
      "Converted longevity10k.ipynb.\n",
      "Converted networks.ipynb.\n",
      "Converted params.ipynb.\n",
      "Converted pubmed.ipynb.\n",
      "Converted roam_utils.ipynb.\n",
      "Converted save.ipynb.\n",
      "Converted semanticscholar_api.ipynb.\n",
      "Converted spec.ipynb.\n",
      "Converted text.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scispacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0c207fc26235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabbreviation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbbreviationDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scispacy'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "#!/usr/bin/env python\n",
    "import functools\n",
    "\n",
    "# import visualise_spacy_pattern\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from operator import itemgetter\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from spacy import displacy\n",
    "from spacy.lemmatizer import ADJ, NOUN, VERB, Lemmatizer\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from typeguard import typechecked\n",
    "\n",
    "# import en_core_sci_md, en_core_sci_sm\n",
    "\n",
    "# tip: Use Code Folding\n",
    "\n",
    "\n",
    "def ents_w_candidates(doc):\n",
    "    return [(ent, candidates_lables(ent)) for ent in doc.ents]\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def candidates_lables(ent: Span) -> List:\n",
    "    \"\"\"aggregates labels from possibly multiple NER models (bc5, bionlpa...)\"\"\"\n",
    "    return [cand[\"label\"] for cand in ent._.annotated]\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def get_ent_from_token(token: Token, doc: Doc) -> List[Span]:\n",
    "    \"\"\"Entities can have multiple tokens\"\"\"\n",
    "    return [ent for ent in doc.ents if ent.start_char <= token.idx <= ent.end_char]\n",
    "\n",
    "\n",
    "def load_sci_pipe(model=\"en_core_sci_md\"):\n",
    "    nlp = spacy.load(model)\n",
    "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "\n",
    "    nlp.add_pipe(abbreviation_pipe)\n",
    "    nlp.add_pipe(merge_entities)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "# this might make trouble since it's a list of spans without the .DOC API methods?\n",
    "def m_reannotate():\n",
    "    \"\"\"Takes deserialized doc.user_data with the doc's custom_attributes data and\n",
    "    sets the ._.annotated attribute again on the entity so it's easier to process later\"\"\"\n",
    "    # strangely two entries with different spans for some entities in\n",
    "    # user_data as in ._.annotated although user_data is generated by ._.annotated\n",
    "    try:\n",
    "        Span.set_extension(\"annotated\", default=[], force=True)\n",
    "    except:\n",
    "        5\n",
    "    # except:\n",
    "    #     for k,v in doc.user_data.items():\n",
    "    #         for ent in doc.ents:\n",
    "    #             #print(k, v, k[2])\n",
    "    #             if ent.start_char == k[2]:\n",
    "    #                 #print(type(v), v)\n",
    "    #                 #converting to list to be consistent with merge_docs and other notebooks ._.annotated\n",
    "    #                 ent._.annotated = list(v)# if type(v) == tuple else []\n",
    "\n",
    "\n",
    "def last_token_of_entity(doc: Doc, token: Token) -> Token:\n",
    "    \"\"\"RECURSIVE. Given a token in an entity, it recurses to the right until it finds a token where the IOB is not Inside (I)\"\"\"\n",
    "    \"\"\" HMGB1-induced -> induced\"\"\"\n",
    "    next_token = token.nbor()\n",
    "    is_end = next_token.ent_iob != 1\n",
    "    if is_end:\n",
    "        return token\n",
    "    else:\n",
    "        return last_token_of_entity(doc, next_token)\n",
    "\n",
    "\n",
    "def negated_ents(doc: Doc) -> List[Span]:\n",
    "    \"\"\"Returns List of all Entities that have a True flag under ._.negex\"\"\"\n",
    "    return [ent for ent in doc.ents if ent._.negex]\n",
    "\n",
    "\n",
    "def show_negex_entities(docs: List[Doc]) -> List[Dict]:\n",
    "    \"\"\"Shows the negated entities of that sentence (estimated by patterns inside negSpacy). Filters out sentences with no negations\"\"\"\n",
    "    ent_pat = {}\n",
    "    for doc in docs:\n",
    "        ent_pat[doc.text] = negated_ents(doc)\n",
    "    return [{k: ent_pat[k]} for k in ent_pat if len(ent_pat[k])]\n",
    "\n",
    "\n",
    "def get_token_and_entities_as_spans(doc: Doc) -> List[Span]:\n",
    "    \"\"\"\n",
    "    Possibly merge_entities does this but...\n",
    "    Helpful if you need a list of token and entities together. Tokens that are part of an entity are merged\n",
    "    Takes a Doc and gives you an iterable that doc.ents or doc[..] can't. It groups token by entity and maintains orginal doc order\"\"\"\n",
    "    spans = []\n",
    "    idx = 0\n",
    "    while idx < len(doc):\n",
    "        print(idx, type(idx))\n",
    "        token = doc[idx]\n",
    "        if token.ent_iob == 3:  # beginning of entity\n",
    "            end_token = last_token_of_entity(doc, token)\n",
    "            print(token, end_token)\n",
    "            spans.append(doc[token.i : end_token.i + 1])\n",
    "            idx = end_token.i + 1  # don't have an endless while loop\n",
    "        else:\n",
    "            idx = idx + 1\n",
    "            spans.append(doc[token.i])\n",
    "    return spans\n",
    "\n",
    "\n",
    "def add_pipes_mutative(nlps, linker):\n",
    "    \"\"\"add pipeline components to every nlp pipeline \"\"\"\n",
    "    for nlp in nlps:  # mutative\n",
    "        abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "        nlp.add_pipe(abbreviation_pipe)\n",
    "        nlp.add_pipe(merge_entities)\n",
    "        nlp.add_pipe(linker)\n",
    "    return nlps\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def get_ont_name(\n",
    "    linker: EntityLinker,\n",
    ") -> str:  # assumes a lot on consistent naming in scispacy\n",
    "    \"GeneOntology or UMLS or MESH\"\n",
    "    return str(linker.kb).split(\".\")[2].split(\" \")[0]\n",
    "\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    \"\"\"setting nested attributes\"\"\"\n",
    "    pre, _, post = attr.rpartition(\".\")\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "\n",
    "def rgetattr(obj, path: str, *default):\n",
    "    \"\"\"\n",
    "    :param obj: Object\n",
    "    :param path: 'attr1.attr2.etc'\n",
    "    :param default: Optional default value, at any point in the path\n",
    "    :return: obj.attr1.attr2.etc\n",
    "    \"\"\"\n",
    "    DELIMITER = \".\"\n",
    "    attrs = path.split(DELIMITER)\n",
    "    try:\n",
    "        return functools.reduce(getattr, attrs, obj)\n",
    "    except AttributeError:\n",
    "        if default:\n",
    "            return default[0]\n",
    "        raise\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def get_entity_diff(\n",
    "    doc: Doc, new_ents_info: List[dict]\n",
    "):  # candidates= [{start, end, label_, label}]\n",
    "    \"\"\"Looks if there is an  entity match at the exact same position of another document (with another pipeline)\n",
    "    . If yes, the new overrides the old. How to merge docs, if it is at all a good idea is a mystery to me\n",
    "    \"\"\"\n",
    "    seen_tokens = set()\n",
    "    new_entities = []\n",
    "    old_entities = doc.ents\n",
    "    for named_ent in new_ents_info:  # every entity\n",
    "        start_char, end_char, label = itemgetter(\"start_char\", \"end_char\", \"label_\")(\n",
    "            named_ent\n",
    "        )\n",
    "        start = get_tokenidx_for_char(doc, start_char)\n",
    "        end = get_tokenidx_for_char(doc, end_char)\n",
    "        #    span = Span(doc, start, end, label=match_id)\n",
    "        #    doc.ents = list(doc.ents) + [span]\n",
    "        # check for end - 1 here because boundaries are inclusive\n",
    "        try:\n",
    "            if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "                entity = Span(doc, start, end, label=label)\n",
    "                new_entities.append(entity)\n",
    "\n",
    "                # compare by token idx\n",
    "                old_entities = [\n",
    "                    e for e in old_entities if not (e.start < end and e.end > start)\n",
    "                ]\n",
    "\n",
    "                # compare by char_offset (should have same results)\n",
    "                # old_entities = [ e for e in old_entities if not (e.start_char < end_char and e.end_char > start_char)]\n",
    "\n",
    "                seen_tokens.update(range(start, end))\n",
    "        except TypeError:\n",
    "            \"mehhh token index is off -- have to do this func on the char_level\"\n",
    "    return old_entities, new_entities\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def merge_named_entities(\n",
    "    docA: Doc, docB: Doc, merge_from_model: str, model_kb_name: str\n",
    "):\n",
    "    \"\"\"Merges two docs and places the kb_ents as extension with the ontology and model name.\n",
    "    So if two entities merge, the old entities can still be found on the object (EXCEPT! Base_doc ...because it is not NER just ER)\n",
    "    \"\"\"\n",
    "    ents_info = extract_named_entities_info(docB)\n",
    "    old_entities, new_entities = get_entity_diff(docA, ents_info)\n",
    "    # print(type(new_entities[0]), new_entities[0]._.kb_ents)\n",
    "    if len(new_entities):\n",
    "        # if(getattr(new_entities[0], \"_.\" + \"model_labels\")\n",
    "        new_entities[0].set_extension(\"annotated\", default=[], force=True)\n",
    "        # new_entities[0].set_extension(merge_from_model, default={}, force=True)\n",
    "\n",
    "        for idx, named_ent in enumerate(new_entities):\n",
    "            kb_ents = ents_info[idx][\"kb_ents\"]\n",
    "            # print(\"kb_ents\", kb_ents)\n",
    "\n",
    "            infos = rgetattr(named_ent, \"_.\" + \"annotated\")\n",
    "\n",
    "            # EVERY entity gets an info object\n",
    "            info = {\n",
    "                \"idx\": idx,\n",
    "                \"kb_ents\": kb_ents,\n",
    "                \"label\": named_ent.label_,\n",
    "                \"model\": model_kb_name,\n",
    "                \"end_char\": named_ent.end_char,\n",
    "                \"start_char\": named_ent.start_char,\n",
    "            }\n",
    "            infos.append(info)\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            info\n",
    "            info[]\n",
    "            info[\"label\"] = named_ent.label_\n",
    "            info[\"end_char\"] = named_ent.end_char\n",
    "            info[\"start_char\"] = named_ent.start_char\n",
    "            \"\"\"\n",
    "\n",
    "            rsetattr(named_ent, \"_.\" + \"annotated\", infos)\n",
    "            # rsetattr(named_ent, \"_.\" + \"labels.\" + merge_from_model, named_ent.label_)\n",
    "            # rsetattr(named_ent, \"_.\" + \"labels\"), {model_kb_name: kb_ents})\n",
    "\n",
    "    # TODO: sets --> check who overwrites whom (only if entities named) --> especially after first model\n",
    "    # aka filter(has_label, old_entities).intersect(new_entities)\n",
    "    return tuple(old_entities) + tuple(new_entities)  # merged_doc\n",
    "\n",
    "\n",
    "def print_table(rows, padding=0):\n",
    "    \"\"\" Print `rows` with content-based column widths. \"\"\"\n",
    "    col_widths = [max(len(str(value)) for value in col) + padding for col in zip(*rows)]\n",
    "    total_width = sum(col_widths) + len(col_widths) - 1\n",
    "    fmt = \" \".join(\"%%-%ds\" % width for width in col_widths)\n",
    "    print(fmt % tuple(rows[0]))\n",
    "    print(\"~\" * total_width)\n",
    "    for row in rows[1:]:\n",
    "        print(fmt % tuple(row))\n",
    "\n",
    "\n",
    "def show_noun_chunks(doc: Doc):\n",
    "    rows = [[\"Chunk\", \".root\", \"root.dep_\", \".root.head\"]]\n",
    "    for chunk in doc.noun_chunks:\n",
    "        rows.append(\n",
    "            [\n",
    "                chunk,  # A Span object with the full phrase.\n",
    "                chunk.root,  # The key Token within this phrase.\n",
    "                chunk.root.dep_,  # The grammatical role of this phrase.\n",
    "                chunk.root.head,  # The grammatical parent Token.\n",
    "            ]\n",
    "        )\n",
    "    print_table(rows, padding=4)\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def doc_has_entity_labels(doc, ent_labels: List[List[str]]):\n",
    "    \"\"\"Checks if a pair of entities (two lists of aliases) show up in a document. For sentence level checks: .... maybe something else\"\"\"\n",
    "    for idx, ent in enumerate(doc.ents):\n",
    "        doc_labels = [\n",
    "            annotation[\"label\"] for ent in doc.ents for annotation in ent._.annotated\n",
    "        ]\n",
    "        return any([label in doc_labels for label in ent_labels[0]]) and any(\n",
    "            [label in doc_labels for label in ent_labels[1]]\n",
    "        )\n",
    "    return False\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def get_merged_docs_for_texts(\n",
    "    texts: List[str], base_nlp, NER_nlps: List, print_every_nth: 25\n",
    ") -> List[Doc]:\n",
    "    print(\n",
    "        \"Merging Named Entities (Chems, Gene, Organism etc.). Later docs in the pipeline overwrite entities from earlier ones\"\n",
    "    )\n",
    "    docs = []\n",
    "    span_overflow_errors = []\n",
    "    for idx, text in enumerate(texts):\n",
    "        doc = base_nlp(text)\n",
    "        for nlp in NER_nlps:  # later nlps overwrite entities of earlier ones\n",
    "            try:\n",
    "                next_doc = nlp(text)\n",
    "                doc.ents = merge_named_entities(doc, next_doc, nlp.meta[\"name\"], \"umls\")\n",
    "            except IndexError:\n",
    "                span_overflow_errors.append(\n",
    "                    \"index : \" + str(idx) + \" --- error for doc: \" + text[:10]\n",
    "                )\n",
    "        docs.append(doc)\n",
    "        if len(docs) % print_every_nth == 0:\n",
    "            print(\"processed \" + len(docs))\n",
    "    [print(errormsg) for errormsg in span_overflow_errors]\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_tokenidx_for_char(doc, char_idx):\n",
    "    \"\"\"\n",
    "    adapted from https://stackoverflow.com/questions/55109468/spacy-get-token-from-character-index\n",
    "    \"\"\"\n",
    "    for index, token in enumerate(doc):\n",
    "        if char_idx > token.idx:\n",
    "            continue\n",
    "        if char_idx == token.idx:\n",
    "            return token.i\n",
    "        if char_idx < token.idx:\n",
    "            return doc[index].i  # here maybe index-1 ...\n",
    "\n",
    "\n",
    "def extract_named_entities_info(doc) -> List[dict]:\n",
    "    \"\"\"Get important props from the spacy entity object\"\"\"\n",
    "    # keys=[\"start\", \"end\", \"label_\"] #span can't be accessed with bracket notation....\n",
    "    ents_info = []\n",
    "    # this is stupid, but Span doesn't do dictionary comprehension API\n",
    "    for ent in doc.ents:\n",
    "        info = {}\n",
    "        info[\"start\"] = ent.start\n",
    "        info[\"end\"] = ent.end\n",
    "        info[\"start_char\"] = ent.start_char\n",
    "        info[\"end_char\"] = ent.end_char\n",
    "        info[\"label_\"] = ent.label_\n",
    "        info[\"kb_ents\"] = ent._.kb_ents\n",
    "        ents_info += [info]\n",
    "    return ents_info\n",
    "\n",
    "\n",
    "def pattern_vis(pattern: List[dict]):\n",
    "    print(\"png display not working.. something stupid about pydot\")\n",
    "    \"\"\" graph = visualise_spacy_pattern.to_pydot(pattern)\n",
    "    # render pydot by calling dot, no file saved to disk\n",
    "    png = graph.create_png(prog=\"dot\")\n",
    "    #graph_file = 'graph1.png'.format()\n",
    "\n",
    "    # treat the dot output string as an image file\n",
    "    sio = BytesIO()\n",
    "    sio.write(png)\n",
    "    sio.seek(0)\n",
    "    img = mpimg.imread(sio)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6,6)\n",
    "\n",
    "    img=mpimg.imread(png)\n",
    "    imgplot = plt.imshow(img,vmin=600)\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "def prep_pattern(pattern: str) -> List[List[str]]:\n",
    "    return [rule.split(\"|\") for rule in pattern.split(\" \")]\n",
    "\n",
    "\n",
    "def add_matches(\n",
    "    vocab,\n",
    "    patterns: List[str],\n",
    "    lemmas=True,\n",
    "    start_ents=None,\n",
    "    end_ents=None,\n",
    "    print_patterns=False,\n",
    "):\n",
    "    # BAAAAD PATTERN! PASSING DOWN VARS (DECORATOR?)\n",
    "    \"\"\"Converts \"prevented|nsubj|START_ENTITY prevented|dobj|END_ENTITY\"\n",
    "    into a pattern that DependencyMatcher class can use\"\"\"\n",
    "    matcher = DependencyMatcher(vocab)\n",
    "    for p in patterns:\n",
    "        pattern = construct_pattern(\n",
    "            p, lemmatize=lemmas, start_ents=start_ents, end_ents=end_ents\n",
    "        )\n",
    "        if print_patterns:\n",
    "            print(pattern, p)\n",
    "        try:  # object of type 'NoneType' has no len() weirdly on some of the 20k dep paths\n",
    "            matcher.add(p, None, pattern)\n",
    "        except:\n",
    "            print(\"error with pattern\", p, \"-->\", pattern)\n",
    "            continue\n",
    "    return matcher\n",
    "\n",
    "\n",
    "def get_lemma(verb, V=True, model=None):  # nlp\n",
    "    if model:\n",
    "        nlp = model\n",
    "    else:\n",
    "        nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "    lemmatizer = nlp.vocab.morphology.lemmatizer  # lemmatizer(\"is\") -> \"be\"\n",
    "    return lemmatizer(verb, VERB)[0]\n",
    "\n",
    "\n",
    "def check_for_non_trees(rules: List[List[str]]):\n",
    "\n",
    "    parent_to_children = defaultdict(list)\n",
    "    seen = set()\n",
    "    has_incoming_edges = set()\n",
    "    for (parent, rel, child) in rules:\n",
    "        seen.add(parent)\n",
    "        seen.add(child)\n",
    "        has_incoming_edges.add(child)\n",
    "        if parent == child:\n",
    "            return None\n",
    "        parent_to_children[parent].append((rel, child))\n",
    "\n",
    "    # Only accept strictly connected trees.\n",
    "    roots = seen.difference(has_incoming_edges)\n",
    "    if len(roots) != 1:\n",
    "        return None\n",
    "\n",
    "    root = roots.pop()\n",
    "    seen = {root}\n",
    "\n",
    "    # Step 2: check that the tree doesn't have a loop:\n",
    "    def contains_loop(node):\n",
    "        has_loop = False\n",
    "        for (_, child) in parent_to_children[node]:\n",
    "            if child in seen:\n",
    "                return True\n",
    "            else:\n",
    "                seen.add(child)\n",
    "                has_loop = contains_loop(child)\n",
    "            if has_loop:\n",
    "                break\n",
    "\n",
    "        return has_loop\n",
    "\n",
    "    if contains_loop(root):\n",
    "        return None\n",
    "\n",
    "    return root, parent_to_children\n",
    "\n",
    "\n",
    "def annotate_NER(model, gpu=False):\n",
    "    \"\"\"\"\"\"\n",
    "    model(gpu)\n",
    "\n",
    "\n",
    "def construct_pattern(\n",
    "    rules: List[List[str]], lemmatize=True, start_ents=None, end_ents=None, log_info=[]\n",
    "):\n",
    "    \"\"\"\n",
    "    Idea: add patterns to a matcher designed to find a subtree in a spacy dependency tree.\n",
    "    Rules are strictly of the form \"CHILD --rel--> PARENT\". To build this up, we add rules\n",
    "    in DFS order, so that the parent nodes have already been added to the dict for each child\n",
    "    we encounter.\n",
    "    \"\"\"\n",
    "    # Step 1: Build up a dictionary mapping parents to their children\n",
    "    # in the dependency subtree. Whilst we do this, we check that there is\n",
    "    # a single node which has only outgoing edges.\n",
    "\n",
    "    if type(rules) is str:\n",
    "        rules = prep_pattern(rules)\n",
    "\n",
    "    if \"dep\" in {rule[1] for rule in rules}:\n",
    "        return None\n",
    "\n",
    "    ret = check_for_non_trees(rules)\n",
    "\n",
    "    if ret is None:\n",
    "        return None\n",
    "    else:\n",
    "        root, parent_to_children = ret\n",
    "\n",
    "    def add_node(parent: str, pattern: List):\n",
    "\n",
    "        for (rel, child) in parent_to_children[parent]:\n",
    "\n",
    "            # First, we add the specification that we are looking for\n",
    "            # an edge which connects the child to the parent.\n",
    "            node = {\n",
    "                \"SPEC\": {\"NODE_NAME\": child, \"NBOR_RELOP\": \">\", \"NBOR_NAME\": parent},\n",
    "            }\n",
    "\n",
    "            # DANGER we can only have these options IF we also match ORTH below, otherwise it's torturously slow.\n",
    "            # token_pattern = {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}}\n",
    "\n",
    "            # Now, we specify what attributes we want this _token_\n",
    "            # to have - in this case, we want to match a certain dependency\n",
    "            # relation specifically.\n",
    "            token_pattern = {\"DEP\": rel}\n",
    "\n",
    "            # Additionally, we can specify more token attributes. So here,\n",
    "            # if the node refers to the start or end entity, we require that\n",
    "            # the word is part of an entity (spacy syntax is funny for this)\n",
    "            # and that the word is a noun, as there are some verbs annotated as \"entities\" in medmentions.\n",
    "\n",
    "            # TODO: variable entities pair matchings\n",
    "            # if child in {\"start_entity\", \"end_entity\", \"START_ENTITY\", \"END_ENTITY\"}:\n",
    "            #   #print(child, \"ahe\")\n",
    "            #   token_pattern[\"ENT_TYPE\"] = {\"IN\": [\"SIMPLE_CHEMICAL\", \"GENE_OR_GENE_PRODUCT\"]}\n",
    "            #   token_pattern[\"POS\"] = \"NOUN\"\n",
    "\n",
    "            if str.lower(child) in {\"start_entity\", \"end_entity\"}:\n",
    "                #!UNCOMMENT BELOW IFF ONLY TOKENS that also have ENTITIES can be in the match-subtree!!\n",
    "                token_pattern[\"ENT_TYPE\"] = {\"NOT_IN\": [\"\"]}\n",
    "\n",
    "                if start_ents and end_ents:\n",
    "                    6\n",
    "                    # TODO! THIS DOESN\"T WORK YET, bc ENT_TYPE looks on label, not ._. our custom attrs\n",
    "                    # token_pattern[\"ENT_TYPE\"] = {\"NOT_IN\": start_ents + end_ents}\n",
    "\n",
    "                token_pattern[\"POS\"] = \"NOUN\"\n",
    "\n",
    "            # If we are on part of the path which is not the start/end entity,\n",
    "            # we want the word to match. This could be made very flexible, e.g matching\n",
    "            # verbs instead, etc.\n",
    "            else:\n",
    "                token_pattern[\"ORTH\"] = child\n",
    "\n",
    "            node[\"PATTERN\"] = token_pattern\n",
    "\n",
    "            pattern.append(node)\n",
    "            add_node(child, pattern)\n",
    "\n",
    "    root_pattern = {\"ORTH\": root}\n",
    "    if lemmatize:\n",
    "        root_pattern = {\"LEMMA\": get_lemma(root)}\n",
    "    pattern = [{\"SPEC\": {\"NODE_NAME\": root}, \"PATTERN\": root_pattern}]\n",
    "    add_node(root, pattern)\n",
    "\n",
    "    assert len(pattern) < 20\n",
    "    return pattern\n",
    "\n",
    "\n",
    "# pattern = [\n",
    "#     {\"SPEC\": {\"NODE_NAME\": \"prevented\"}, \"PATTERN\": {\"ORTH\": \"prevented\"}},\n",
    "#     {\"SPEC\": {\"NODE_NAME\": \"start_entity\", \"NBOR_RELOP\": \">\", \"NBOR_NAME\": \"prevented\"}, \"PATTERN\": {\"DEP\": \"nsubj\"}},\n",
    "#     {\"SPEC\": {\"NODE_NAME\": \"release\", \"NBOR_RELOP\": \">\", \"NBOR_NAME\": \"prevented\"}, \"PATTERN\": {\"DEP\": \"dobj\", \"ORTH\":\"release\"}},\n",
    "#     {\"SPEC\": {\"NODE_NAME\": \"end_entity\", \"NBOR_RELOP\": \">\", \"NBOR_NAME\": \"release\"}, \"PATTERN\": {\"DEP\": \"nmod\"}},\n",
    "\n",
    "# ]\n",
    "\n",
    "\n",
    "# @typechecked\n",
    "def match_texts(matcher: DependencyMatcher, docs: List[Union[Doc, str]], nlp) -> dict:\n",
    "    output = {}\n",
    "    if type(docs[0]) == str:\n",
    "        docs = [nlp(text) for text in docs]\n",
    "    for doc in docs:\n",
    "        matches = matcher(doc)\n",
    "        # print(matches, 'match')\n",
    "        for match_id, ms in matches:\n",
    "            rule_id = nlp.vocab.strings[\n",
    "                match_id\n",
    "            ]  # get name of the pattern, i.e. 'x|prevented|y'\n",
    "\n",
    "            if len(ms):\n",
    "                start = min(ms[0])\n",
    "                end = max(ms[0])\n",
    "\n",
    "                sents = [doc[start].sent]\n",
    "                # in case there a relation spans two sentences ... usually not\n",
    "                if doc[start].sent != doc[end].sent:\n",
    "                    sents = [doc[start].sent, doc[end].sent]\n",
    "\n",
    "                span = doc[start : end + 1]\n",
    "\n",
    "                doc_match = {\n",
    "                    \"doc_idx\": len(docs) - 1,\n",
    "                    \"span\": span.text or \"---\",\n",
    "                    \"sents\": sents,\n",
    "                    \"matches\": ms,\n",
    "                    \"sent_ents\": [sent.ents for sent in sents],\n",
    "                }\n",
    "                if rule_id not in output:\n",
    "                    output[rule_id] = []\n",
    "                # print(\"HIT\", doc_match)\n",
    "                output[rule_id].append(doc_match)\n",
    "    return output\n",
    "\n",
    "\n",
    "# show_tabs(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
