{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from varname import varname, nameof\n",
    "\n",
    "from numpy import ndarray\n",
    "from pandas import DataFrame, Series\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from spacy_stanza import StanzaLanguage\n",
    "from fastcore.test import test_eq\n",
    "\n",
    "from functools import wraps\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gsheet_to_df(worksheet) -> DataFrame:\n",
    "    df = DataFrame(worksheet.get_all_values())\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.iloc[1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "* use marshmallow python lib for serialization?\n",
    "* more testing on auto_coerce!!! ... it's only working for some right now\n",
    "* conform instead of typed now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECS inspired by Malli for Clojure \n",
    "\n",
    "#export\n",
    "\n",
    "TRANSFORMER = {\n",
    "    \"name\": \"TRANSFORMER\"\n",
    "}\n",
    "SENTENCE_TRANSFORMER = {\n",
    "    \"name\":\"SENTENCE_TRANSFORMER\"\n",
    "}\n",
    "PUBMED_IDS = {\n",
    "    \"name\": \"PUBMED_IDS\"\n",
    "}\n",
    "PUBMED_CONTENT = {\n",
    "    \"name\": \"PUBMED_CONTENT\"\n",
    "}\n",
    "EMAIL = {\n",
    "    \"name\": \"EMAIL\"\n",
    "}\n",
    "\n",
    "# JSON = {\n",
    "#     \"re\": \"(.*?)\\.(json)$\"\n",
    "# }\n",
    "\n",
    "# TEXT = {\n",
    "#     \"re\": \"(.*?)\\.(txt)$\"\n",
    "# }\n",
    "\n",
    "SPACY_MODEL = {\n",
    "    \"re\": \"(zh|da|nl|en|fr|de|el|it|ja|lt|nb|pl|pt|ro|es|xx)[_(core|ent|ner)_(web|news|wiki|sci|craft|jnlpba|bc5cdr|bionlp13cg)_(sm|md|lg)]*$\"\n",
    "}\n",
    "STANZA_MODEL = {\n",
    "    \"re\": \"stanza\",\n",
    "    \"doc\": \"Stanford's Stanza Model\",\n",
    "    \"options\": [\"stanza1\", \"stanza2\"]\n",
    "}\n",
    "\n",
    "\n",
    "PUBMED_IDS = {\"re\": \".*pubmed.ncbi.nlm.nih.gov.*\"}\n",
    "GSHEET = {\"re\": \".*docs.google.com\\/spreadsheets.*\"}\n",
    "    \n",
    "TRANSFORMER = {\"re\": \"TRANSFORMER:.*\"}\n",
    "HTML_TAG = {\n",
    "    \"name\": \"HTML_TAG\"\n",
    "}\n",
    "URL = {\n",
    " \"re\": '\\(?((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\\)?'   \n",
    "}\n",
    "\n",
    "\n",
    "# for t in TYPES:  # PYTHON MAGIC\n",
    "#     exec(\"%s=str('%s')\" % (t, t))\n",
    "\n",
    "\"\"\"\n",
    "? Dynamically create Types in NameSpace aka TRANSFORMER = \"TRANSFORMER\"\n",
    "\"\"\"\n",
    "SPEC = [GSHEET, JSON, TEXT, TRANSFORMER, SENTENCE_TRANSFORMER, PUBMED_CONTENT, PUBMED_IDS, SPACY_MODEL, STANZA_MODEL, EMAIL, HTML_TAG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if {}:\n",
    "    print(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([True for s in range(len(SPEC))], [type(schema) == dict and bool(schema) for schema in SPEC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def infer_type(form, SPEC=SPEC):\n",
    "    \"\"\" What types match this shape? \"\"\"\n",
    "    \n",
    "    schemas_with_re = [schema for schema in SPEC if schema.get(\"re\")]\n",
    "    schemas_with_validate = [schema for schema in SPEC if schema.get(\"validate\")]\n",
    "    \n",
    "    match=[]\n",
    "    \n",
    "    if (type(form) == str):\n",
    "        match = [schema for schema in schemas_with_re if re.compile(schema[\"re\"]).match(form)]\n",
    "\n",
    "#     else:\n",
    "#         match = [schema for schema in SPEC if form == schema] #direct {} compare \n",
    "\n",
    "    if len(match) > 1:\n",
    "        raise Exception(\n",
    "            \"Found multiple inferences for the shape you put in. Please put the input_type =EMAIL or something as a keyword argument. MAKE SURE that all schemas are UNIQUELY IDENTIFYABLE\"\n",
    "        )\n",
    "\n",
    "    if len(match) == 0:\n",
    "        print(\"No Match found in type inference, returning None\")\n",
    "        return None\n",
    "    return match[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': '(.*?)\\\\.(json)$'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_type(\"somejason.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(infer_type(\"docs.google.com/spreadsheets.2\"), GSHEET)\n",
    "test_eq(infer_type(\"en_ner_bionlp13cg_md\"), SPACY_MODEL)\n",
    "test_eq(infer_type(\"somejason.json\"), JSON)\n",
    "#test_eq(infer_type(\"distilbert-base-nli-mean-tokens\"), SENTENCE_TRANSFORMER)\n",
    "#test_eq(infer_type(SENTENCE_TRANSFORMER), SENTENCE_TRANSFORMER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _transformer_to_sentence_transformer(transformer_model):\n",
    "    pooling_model = models.Pooling(\n",
    "        transformer_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True,\n",
    "        pooling_mode_cls_token=True,\n",
    "        pooling_mode_max_tokens=False,\n",
    "    )\n",
    "\n",
    "    return SentenceTransformer(modules=[transformer_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for name, value in globals().items():\n",
    "#     if value is STANZA_MODEL:\n",
    "#         print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varname(STANZA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Source, Target) -> Transform Function\n",
    "#export\n",
    "TRANSFORMS = {\n",
    "    #!!! DO NOT USE Tuple[str] or any of these inferior BS pythonista types. They don't work and are an abomination\n",
    "    (\"SPACY_MODEL\", \"STANZA_MODEL\"): StanzaLanguage,\n",
    "    (str, list): lambda string: [string],\n",
    "    (tuple, list): list,\n",
    "    (\"GSHEET\", DataFrame): gsheet_to_df,\n",
    "    (Series, list): Series.to_list,\n",
    "    (ndarray, list): list,\n",
    "    (\"TRANSFORMER\", \"SENTENCE_TRANSFORMER\"): _transformer_to_sentence_transformer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convert(shape, source, target):\n",
    "    \"\"\"\n",
    "    Converts an object from source type to target type\n",
    "    \"\"\"\n",
    "    s = source if source==str or list or tuple or dict else varname(source)\n",
    "    t = target if target==str or list or tuple or dict else varname(target)\n",
    "    convert_func = TRANSFORMS[(s, t)]\n",
    "    return convert_func(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list == str or list or tuple or dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(convert((\"aa\", \"bb\"), source=tuple, target=list), [\"aa\", \"bb\"])\n",
    "test_eq(convert((\"aa\", \"bb\"), source=tuple, target=list), [\"aa\", \"bb\"])\n",
    "#test_eq(type(convert(load(\"en\"), source=\"SPACY_MODEL\", target=\"STANZA_MODEL\")), StanzaLanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "def auto_coerce(func, TRANSFORMS=TRANSFORMS):\n",
    "    \"\"\"\n",
    "    DECORATOR: Right now we only do kwargs.\n",
    "    Use this solution to get away from it: https://docs.python.org/3/library/inspect.html#introspecting-callables-with-the-signature-object\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #def a(r:list): return r\n",
    "    #a.__annotations__ == {'r': list}\n",
    "    #args, kwargs == () {'r': [1, 23]}\n",
    "    \n",
    "    @wraps(func) \n",
    "    def wrapped(*args, **kwargs):\n",
    "        #print(args, kwargs)\n",
    "        annotations = func.__annotations__\n",
    "        updated_kwargs = copy.deepcopy(kwargs)\n",
    "        if not len(args):\n",
    "            for k, v in updated_kwargs.items():\n",
    "                \n",
    "                #if there's no schema match\n",
    "                spec = infer_type(v)\n",
    "                inputdata_type = nameof(spec) if spec else type(v)\n",
    "                \n",
    "                anno_type = annotations.get(k)\n",
    "                print(inputdata_type,\"aaasd\", anno_type)\n",
    "                #look for convert function or do nothing\n",
    "                convert_func = TRANSFORMS.get((inputdata_type, anno_type), identity)\n",
    "                \n",
    "                updated_kwargs[k] = convert_func(v)\n",
    "\n",
    "        return_value = func(*args, **updated_kwargs)\n",
    "        TRANSFORMS_function = TRANSFORMS.get(\n",
    "            (type(return_value), annotations.get(\"return\")), identity\n",
    "        )\n",
    "        return TRANSFORMS_function(return_value)\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Match found in type inference, returning None\n",
      "<class 'numpy.ndarray'> aaasd <class 'list'>\n",
      "No Match found in type inference, returning None\n",
      "<class 'tuple'> aaasd <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def a(x:list): return x\n",
    "def b(x:SENTENCE_TRANSFORMER): return x\n",
    "\n",
    "test_eq(type(auto_coerce(a)(x=np.zeros(5))), list)\n",
    "test_eq(type(auto_coerce(a)(x=(\"a\", \"b\"))), list)\n",
    "#test_eq(type(auto_coerce(b)(x=(\"a\", \"b\"))), list)\n",
    "\n",
    "#auto_coerce(b)(x=(\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted aws_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted load.ipynb.\n",
      "Converted roam_utils.ipynb.\n",
      "Converted semanticscholar_api.ipynb.\n",
      "Converted spec.ipynb.\n",
      "Converted text.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
