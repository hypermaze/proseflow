{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import networkx as nx\n",
    "from nltk import Tree\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "\n",
    "# doc = nlp(u'Convulsions that occur after DTaP are caused by a fever, and fever may cause headache.')\n",
    "def get_sdp_path(doc, subj: int, obj: int):\n",
    "    \"\"\"\n",
    "    'Convulsions that occur after DTaP are caused by a fever, and fever may cause headache.'\n",
    "       ----> [Convulsions, caused, by, fever]\n",
    "\n",
    "    Get shortest dependency path without networkx lib. Usues spacy's LCA (lowest common ancestor) matrix\n",
    "    Adapted from:https://towardsdatascience.com/find-lowest-common-ancestor-subtree-and-shortest-dependency-path-with-spacy-only-32da4d107d7a\n",
    "    \"\"\"\n",
    "\n",
    "    lca = doc.get_lca_matrix()[subj, obj]\n",
    "\n",
    "    current_node = doc[subj]\n",
    "    subj_path = [current_node]\n",
    "    if lca != -1:\n",
    "        if lca != subj:\n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                subj_path.append(current_node)\n",
    "            subj_path.append(current_node.head)\n",
    "\n",
    "    current_node = doc[obj]\n",
    "    obj_path = [current_node]\n",
    "    if lca != -1:\n",
    "        if lca != obj:\n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                obj_path.append(current_node)\n",
    "        obj_path.append(current_node.head)\n",
    "\n",
    "    return subj_path + obj_path[::-1][1:]\n",
    "\n",
    "\n",
    "# Load spacy's dependency tree into a networkx graph\n",
    "def get_edges(doc: Union[Doc, List[Span]]):\n",
    "    \"\"\"Use get_edges_unique if you want object +id\"\"\"\n",
    "    edges = []\n",
    "\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((token, child))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_edges_unique(doc: Union[Doc, List[Span]]):\n",
    "    \"\"\"\"\"\"\n",
    "    edges = []\n",
    "\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            # if you want text instead of actual object!\n",
    "            # edges.append(('{0}-{1}'.format(token.text, token.i),\n",
    "            #'{0}-{1}'.format(child.text, child.i)))\n",
    "\n",
    "            edges.append(((token, token.i), (child, child.i)))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_verb(tokens: List[Token]):\n",
    "    for token in tokens:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            return token\n",
    "\n",
    "\n",
    "# TODO either make a dict or read context off of tokens later\n",
    "def triples_from_pairs(pairs: List[Tuple[Doc, Doc]], G, doc: Doc, relation_fn=get_verb):\n",
    "    \"\"\"takes two entities and finds a relation between them (relation_fn) looking at the\n",
    "    shortest path. Edit: Too simple. Instead of assuming it's the verb, lookup in the dependency patterns (GNBR)\"\"\"\n",
    "    triples = []\n",
    "    for pair in pairs:\n",
    "        i1, i2 = pair\n",
    "        source, target = doc[i1], doc[i2]\n",
    "        SDP = nx.shortest_path(\n",
    "            G, source, target\n",
    "        )  # previous issue: we pass networkx the tokens, but when we give .ents ... we have spans and it doesn't recognize\n",
    "        relation = relation_fn(SDP)\n",
    "        if relation:\n",
    "            triples = triples + [(relation, source, target)]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def is_redundant(value, store):\n",
    "    mirror = (value[1], value[0])\n",
    "    is_same = value[1] is value[0]\n",
    "    return is_same or (value in store) or (mirror in store)\n",
    "\n",
    "\n",
    "def pair_down(all_pairs):\n",
    "    token_str = [\n",
    "        (str(t[0]), str(t[1])) for t in all_pairs\n",
    "    ]  # easier to compare str than tokens\n",
    "    dist = []\n",
    "    for value in token_str:\n",
    "        # print(dist, value)\n",
    "        if is_redundant(value, dist):\n",
    "            value = False\n",
    "        dist = dist + [value]\n",
    "\n",
    "    return [pair for idx, pair in enumerate(all_pairs) if dist[idx] != False]\n",
    "\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.tag_, tok.dep_])\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "\n",
    "# command = \"Submit debug logs to project lead today at 9:00 AM\"\n",
    "# en_doc = en_nlp(u'' + command)\n",
    "# [to_nltk_tree(sent.root).pretty_print() for sent in en_doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-summit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
