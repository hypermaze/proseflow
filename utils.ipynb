{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f8eff8ab7cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtypeguard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypechecked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydash'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from typing import List\n",
    "from fastcore.basics import typed\n",
    "from fastcore.test import *\n",
    "from toolz import thread_first, thread_last\n",
    "import proseflow.text as txt\n",
    "\n",
    "from functools import reduce\n",
    "from typing import Iterable, List, Union\n",
    "\n",
    "from pydash import get\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# TODO: [Markus] : Walk or Tree-seq implementation\n",
    "# @typechecked\n",
    "def tree_select_kv(data, paths, keys_wanted):\n",
    "    flat_dict = {}\n",
    "    for path in paths:\n",
    "        # for keyseq in path:\n",
    "        # print(keyseq)\n",
    "        # print(path, keyseq)\n",
    "        leaf_key = None\n",
    "        if path[-1:]:\n",
    "            leaf_key = path[-1:][0]\n",
    "\n",
    "        # print(path, leaf_key)\n",
    "        if leaf_key in keys_wanted:\n",
    "            deep_dot_path = deep_path_from_keysequence(path)\n",
    "            # print(deep_dot_path, keyseq, leaf_key)\n",
    "            value = get(data, deep_dot_path)\n",
    "            key = leaf_key\n",
    "            flat_dict[key] = value\n",
    "    return flat_dict\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def deep_path_from_keysequence(keysequence: list) -> str:\n",
    "    return reduce(lambda path, part: path + \".\" + str(part), keysequence, \"\")\n",
    "\n",
    "\n",
    "def get_paths(d: dict) -> Iterable[str]:\n",
    "    \"\"\"Given a dict, it returns an array with all paths\n",
    "    Example return: ['PubmedArticle', 0, 'MedlineCitation']\n",
    "    \"\"\"\n",
    "    q = [(d, [])]\n",
    "    while q:\n",
    "        n, p = q.pop(0)\n",
    "        yield p\n",
    "        if isinstance(n, dict):\n",
    "            for k, v in n.items():\n",
    "                q.append((v, p + [k]))\n",
    "        elif isinstance(n, list):\n",
    "            for i, v in enumerate(n):\n",
    "                q.append((v, p + [i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tabulate import tabulate\n",
    "def show_tabs(doc):\n",
    "    print(tabulate([\n",
    "            [token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop] \n",
    "            for token in doc], headers=[\"token\", \"lemma\", \"POS\", \"Tag\", \"DEP\", \"shape\", \"is_alpha\", \"is_stop\"]\n",
    "  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [2, 3], [3, 4], [4, 5], [5, 5], [5, 6], [6, 7]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "def take_while(fn, coll):\n",
    "    \"\"\"Yield values from coll until fn is False\"\"\"\n",
    "    for e in coll:\n",
    "        if fn(e):\n",
    "            yield e\n",
    "        else:\n",
    "            return\n",
    "\n",
    "def partition(n, coll, step=None):\n",
    "    return take_while(lambda e: len(e) == n,\n",
    "        (coll[i:i+n] for i in range(0, len(coll), step or n)))\n",
    "\n",
    "def partition_all(n, coll, step=None):\n",
    "    return (coll[i:i+n] for i in range(0, len(coll), step or n))\n",
    "\n",
    "[*partition(2, [1, 2,3 ,4,5, 5,6,7], 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typed\n",
    "def create_embedding_files_for_visualization(metadata, vectors, metadata_headers=None):\n",
    "    \"\"\" Create embedding files for visualization. Sentences can be any kind of metadata \"\"\"\n",
    "    metadata = [*metadata]\n",
    "    assert len(metadata) == len(vectors)\n",
    "\n",
    "    vectors_filepath = f\"/results/vectors.tsv\"\n",
    "    metadata_filepath = f\"results/metadata.tsv\"\n",
    "\n",
    "    out_vectors = open(vectors_filepath, \"w\", encoding=\"utf-8\")\n",
    "    out_metadata = open(metadata_filepath, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    # Meta File Header\n",
    "    if metadata_headers:\n",
    "        out_metadata.write(\"\\t\".join(metadata_headers) + \"\\n\")\n",
    "\n",
    "    for i in range(len(vectors)):\n",
    "        out_metadata.write(\"\\t\".join(metadata[i]) + \"\\n\")\n",
    "        out_vectors.write(\"\\t\".join([str(x) for x in vectors[i]]) + \"\\n\")\n",
    "\n",
    "    out_vectors.close()\n",
    "    out_metadata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pipe(*funcs:List[callable], thread=\"first\"):\n",
    "    thread = thread_first if thread == \"first\" else thread_last\n",
    "    return lambda data: thread(data, *funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_func in txt.STRING_FUNCS: #PYTHON MAGIC\n",
    "    exec(\"%s=getattr(str, s_func)\" %s_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence = pipe(strip,\n",
    "                      lower)\n",
    "\n",
    "test_eq(\"this is a test\", clean_sentence(\"   THIS iS a TEsT  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dedupe_conseq(coll):\n",
    "    \"\"\"\n",
    "    Returns a generator of the elements of coll with consecutive duplicates removed.\n",
    "    \"\"\"\n",
    "    initial = True\n",
    "    prev = None\n",
    "    for e in coll:\n",
    "        if initial or e != prev:\n",
    "            initial = False\n",
    "            yield e\n",
    "        prev = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([*dedupe_conseq([1,2,3,4,5,5,5,3])], [1, 2, 3, 4, 5, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_seq(has_branch, get_children, root):\n",
    "    \"\"\"\n",
    "    Returns a generator of the nodes in a tree, via a depth-first walk.\n",
    "    ``has_branch`` must be a function of one argument that returns ``True`` if\n",
    "    passed a node that can have children (but may not). ``get_children`` must\n",
    "    be a function of one argument that returns an iterable of the children.\n",
    "    Will only be called on nodes for which ``has_branch`` returns true.\n",
    "    ``root`` is the root node of the tree.\n",
    "    \"\"\"\n",
    "    yield root\n",
    "    if has_branch(root):\n",
    "        for child in get_children(root):\n",
    "            for subchild in tree_seq(has_branch, get_children, child):\n",
    "                yield subchild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 3, 'e': {'f': 6, 'g': 7}}, 'a', 'b', 'e']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"a\": 1, \"b\": 3,\"e\" :{\"f\": 6, \"g\": 7}}\n",
    "[*tree_seq(lambda n: type(n) == dict, lambda x : x, d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted aws_utils.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted load.ipynb.\n",
      "Converted roam_utils.ipynb.\n",
      "Converted semanticscholar_api.ipynb.\n",
      "Converted spec.ipynb.\n",
      "Converted text.ipynb.\n",
      "Converted utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
