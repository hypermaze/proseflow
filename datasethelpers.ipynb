{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasethelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from more_itertools import partition\n",
    "\n",
    "from proseflow.text import construct_pattern\n",
    "\n",
    "GNBR_PATH = \"../data/biomedrel/\"\n",
    "CAUSE_BINARY_PATH = \"../data/cause_binary.csv\"\n",
    "WIKIMED_PATH = \"../data/wikimed.json\"\n",
    "PTB_BRACKETS = {\n",
    "    \"-LRB-\": \"(\",\n",
    "    \"-RRB-\": \")\",\n",
    "    \"-LCB-\": \"{\",\n",
    "    \"-RCB-\": \"}\",\n",
    "    \"-LSB-\": \"[\",\n",
    "    \"-RSB-\": \"]\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_csv(path):\n",
    "    with open(path, newline=\"\") as csvfile:\n",
    "        DS = csv.DictReader(csvfile)\n",
    "        return [*DS]\n",
    "\n",
    "\n",
    "class CauseBinary:\n",
    "    @staticmethod\n",
    "    def get(path=CAUSE_BINARY_PATH):\n",
    "        get_csv(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_causal(cause_ds) -> [List, List]:\n",
    "        \"\"\"Splits Dataset into the two categories: Causality in sentence, or NOT\"\"\"\n",
    "        causal_sents, non_causal_sents = partition(\n",
    "            lambda d: int(d[\"Annotated_Causal\"]) == 0, cause_ds\n",
    "        )\n",
    "        return [*causal_sents], [*non_causal_sents]\n",
    "\n",
    "\n",
    "class WikiMed:\n",
    "    @staticmethod\n",
    "    def get(path=WIKIMED_PATH):\n",
    "        with open(\"../data/wikimed.json\") as f:\n",
    "            wm = [json.loads(d) for d in [*f]]\n",
    "            return wm\n",
    "\n",
    "\n",
    "class GNBR:\n",
    "    @staticmethod\n",
    "    def get_distributions(FILE_PATH):\n",
    "        with open(\n",
    "            FILE_PATH, \"r\"\n",
    "        ) as f:\n",
    "            l = f.readlines()  # bad and inefficient....\n",
    "            headers = l[0].strip().split(\"\\t\")[1:]\n",
    "    #         # headers = next(f).strip().split(\"\\t\")[1:] --this did return [] most of the time\n",
    "    #         print(\n",
    "    #             headers,\n",
    "    #             \"headers\",\n",
    "    #             \"\"\"\n",
    "    #                         chemical-gene\n",
    "    #                 (A+) agonism, activation\n",
    "    #                 (A-) antagonism, blocking\n",
    "    #                 (B) binding, ligand (esp. receptors)\n",
    "    #                 (E+) increases expression/production\n",
    "    #                 (E-) decreases expression/production\n",
    "    #                 (E) affects expression/production (neutral)\n",
    "    #                 (N) inhibits\n",
    "    #                         \"\"\",\n",
    "    #         )\n",
    "            distributions = {}\n",
    "            # incredibly dumb way of doing this because the fileread IO is buggy or just bad\n",
    "            for line in l[1:]:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "\n",
    "                distributions[line[0].lower()] = [float(val) for val in line[1:]]\n",
    "        return distributions\n",
    "#         \n",
    "#        \n",
    "    @staticmethod\n",
    "    def get_data(FILE_PATH):\n",
    "        with open(FILE_PATH,\"r\") as data:\n",
    "            data_headers = [\n",
    "                \"pmid\",\n",
    "                \"sent\",\n",
    "                \"ent1\",\n",
    "                \"ent1_offset\",\n",
    "                \"ent2\",\n",
    "                \"ent2_offset\",\n",
    "                \"ent1_raw\",\n",
    "                \"ent2_raw\",\n",
    "                \"ent1_canonical\",\n",
    "                \"ent2_canonical\",\n",
    "                \"ent1_type\",\n",
    "                \"ent2_type\",\n",
    "                \"dep\",\n",
    "                \"sent\",\n",
    "            ]\n",
    "\n",
    "            lines = [\n",
    "                {k: (v.lower() if k==\"dep\" else v) for k, v in zip(data_headers, line.strip().split(\"\\t\"))}\n",
    "                for line in data\n",
    "            ]\n",
    "            return lines\n",
    "\n",
    "    @staticmethod\n",
    "    def fixup():\n",
    "        \"\"\"GNBR paper (Stanford Dependencies?) isn't fully mapped on UD that spacy uses\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def supports_what(dep_path: str) -> Dict:\n",
    "        \"\"\"Maps and filters the GNBR support theme dict for fast\n",
    "        feedback without you having to lookup the paper all the time\"\"\"\n",
    "        return 5\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_and_parse(sent: str, nlp, PTB_BRACKETS=PTB_BRACKETS):\n",
    "        \"\"\" -LRB- something -RRB- ===> (something) \"\"\"\n",
    "        tokens = sent.strip().split(\" \")\n",
    "\n",
    "        new = []\n",
    "        for token in tokens:\n",
    "            new_token = PTB_BRACKETS.get(token, None)\n",
    "            if new_token is None:\n",
    "                new.append(token)\n",
    "            else:\n",
    "                new.append(new_token)\n",
    "        return nlp(\" \".join(new))\n",
    "\n",
    "    def unambigous_support(support: Dict) -> bool:\n",
    "        \"\"\"Is the support mixed for many themes or pretty focused?\"\"\"\n",
    "        return max(support) / sum(support) > 0.8\n",
    "\n",
    "    @property\n",
    "    def THEMES():\n",
    "        return \"\"\"\n",
    "                Chemical-gene\n",
    "                    A+ \tAgonism, activation \t                        3a \t6+\n",
    "                    A− \tAntagonism, blocking \t \t                    6–\n",
    "                    B \tBinding, ligand (esp. receptors) \t \t        14–16\n",
    "                    E+ \tIncreases expression/production \t \t        8+, 9+\n",
    "                    E− \tDecreases expression/production \t \t        8–, 9–, 10\n",
    "                    E \tAffects expression/production (neutral) \t \t8, 9, 11a\n",
    "                    N \tInhibits \t \t                                3\n",
    "                Gene-chemical\n",
    "                    O \tTransport, channels \t                        3a \t19, 21\n",
    "                    K \tMetabolism, pharmacokinetics \t \t            11c\n",
    "                    Z \tEnzyme activity \t \t                        20\n",
    "                Chemical-disease\n",
    "                    T \tTreatment/therapy (incl. investigatory) \t    3b \t8g, 8h, 9\n",
    "                    C \tInhibits cell growth (esp. cancers) \t \t    2, 3\n",
    "                    Sa \tSide effect/adverse event \t \t                6, 15, 16\n",
    "                    Pr \tPrevents, suppresses \t \t                    1, 9, 21, 24, 28\n",
    "                    Pa \tAlleviates, reduces \t \t                    26, 30\n",
    "                    J \tRole in pathogenesis \t \t                    20\n",
    "                Disease-chemical\n",
    "                    Mp \tBiomarkers (progression) \t                    3b \t18, 19\n",
    "                Gene-disease\n",
    "                    U \tCausal mutations \t                            3c \t14\n",
    "                    Ud \tMutations affect disease course \t \t        13\n",
    "                    D \tDrug targets \t \t                            10, 12\n",
    "                    J \tRole in pathogenesis \t \t                    2h, 4, 6, 8, 9\n",
    "                    Te \tPossible therapeutic effect \t \t            2j, 3\n",
    "                    Y \tPolymorphisms alter risk \t \t                22, 26, 27\n",
    "                    G \tPromotes progression \t \t                    29\n",
    "\n",
    "                Disease-gene \tMd \tBiomarkers (diagnostic) \t        3c \t5, 7\n",
    "                    X \tOverexpression in disease \t \t                15, 17, 30\n",
    "                    L \tImproper regulation linked to disease \t \t    18, 19, 21\n",
    "\n",
    "                Gene–gene \tB \tBinding, ligand (esp. receptors) \t    3d \t10\n",
    "                    W \tEnhances response \t \t                        13\n",
    "                    V+ \tActivates, stimulates \t \t                    14, 16\n",
    "                    E+ \tIncreases expression/production \t \t        21, 22\n",
    "                    E \tAffects expression/production (neutral) \t \t7, 17\n",
    "                    I \tSignaling pathway \t \t                        24\n",
    "                    H \tSame protein or complex \t \t                25\n",
    "                    Rg \tRegulation \t \t                                28, 30\n",
    "                    Q \tProduction by cell population \t \t            1, 2, 6\n",
    "                \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_patterns(dist) -> List[str]:\n",
    "        \"\"\"the construct pattern method returns null if not valid (if dep path is not a DAG?)\"\"\"\n",
    "        valid_count = 0\n",
    "        valids = {}\n",
    "        for dep, values in dist.items():\n",
    "            if construct_pattern(dep) is None:\n",
    "                continue\n",
    "\n",
    "            valids[dep] = values\n",
    "            valid_count += 1\n",
    "\n",
    "        all = len(dist.items())\n",
    "        print(\n",
    "            \"{} of {} patterns (DSP) are valid. That is {} %\".format(\n",
    "                valid_count, all, valid_count / all\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return valids\n",
    "\n",
    "    def peak_ratio(coll: List[Union[int, float]]) -> float:\n",
    "        \"\"\"Distribution measure: 1 if there's one value, less if there is many\"\"\"\n",
    "        return max(coll) / (sum(coll) + 0.000001)\n",
    "\n",
    "    def strongest_support(dist):\n",
    "        counter = Counter()\n",
    "        for dep, values in dist.items():\n",
    "            counter[dep] = sum(list(values.values()))\n",
    "        return counter\n",
    "\n",
    "    def path_lengths(dist):\n",
    "        path_lengths = Counter()\n",
    "        for dep, values in dist.items():\n",
    "            length = len(dep.split(\" \"))\n",
    "            path_lengths[length] += 1\n",
    "        return path_lengths\n",
    "\n",
    "    def path_count(data) -> Counter:\n",
    "        \"\"\"Counts how often a path appears in the data file\"\"\"\n",
    "        print(\"Counting \" + str(len(data)) + \" dependency paths\")\n",
    "        path_count = Counter()\n",
    "        for d in data:\n",
    "            path_count[d[\"dep\"].lower()] += 1\n",
    "        return path_count\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_dep_path(dep_string: str):\n",
    "\n",
    "        rules = [rule.split(\"|\") for rule in dep_string.split(\" \")]\n",
    "\n",
    "        for triple in rules:\n",
    "\n",
    "            if triple[0] in PTB_BRACKETS:\n",
    "                triple[0] = PTB_BRACKETS[triple[0]]\n",
    "\n",
    "            if triple[2] in PTB_BRACKETS:\n",
    "                triple[2] = PTB_BRACKETS[triple[2]]\n",
    "\n",
    "            if triple[1] == \"nsubj:xsubj\":\n",
    "                triple[1] = \"nsubj\"\n",
    "\n",
    "            if triple[1] == \"nsubjpass:xsubj\":\n",
    "                triple[1] = \"nsubjpass\"\n",
    "        return rules\n",
    "\n",
    "\n",
    "# PBT - like random search - starts by training many neural networks in parallel with random hyperparameters. But instead of the networks training independently, it uses information from the rest\n",
    "# of the population to refine the hyperparameters and direct computational resources to models which show promise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-dictionary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proseflow",
   "language": "python",
   "name": "proseflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
